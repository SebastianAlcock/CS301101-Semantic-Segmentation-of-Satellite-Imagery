### Nicholas Gutierrez & Sebastian Alcock
### CS301-101 - Introduction to Data Science
### December 4th, 2022

# Milestone 4: Model Compression and Knowledge Distillation

#### For the final milestone in our semantic segmentation project, our goal was to perform model compression, a process that optimizes models with the intent of allowing them to run when access to GPU accelerators or a high memory is not available. To perform model compression, there are two methods we will be implementing: pruning and quantization. Model pruning is a method that removes excess weight from a model by zeroing out any parameters that are found to be redundant or unimportant to the training of the model, allowing it to take up less space. After pruning the excess parameters from our model, we will then perform quantization. Quantization compresses models by reducing the number of bits that represent weights and activation, reducing computation time and space without major loss to the model’s accuracy. Through quantization, a model’s features can be compressed to sizes as small as 4:1 bit-ratio; but for our model, we will use Tensorflow’s liteConverter function to compress our model in such a way that each parameter and activation is 8 bits. Performing these two techniques will allow our model to be functional on less powerful computers with a minimal decrease in performance. Additionally, it is worth noting that a third technique to improve the model called knowledge distillation is possible. This method creates a smaller pruned model called the student which mimics the original model, the teacher, allowing it to get similar results while being at its compressed size. However, these functions are incompatible with TensorFlow Keras API, on which our model is built. Therefore, for this milestone, we will just be performing pruning and quantization. If we were to use knowledge distillation, our trained model would help create a smaller model to make similar predictions within less space, which still builds upon the two techniques that we will be utilizing.

#### To discuss how we utilized these methods further in-depth, starting with the pruning, we first defined a set of parameters to tell the function how much weight to shed from the model. We defined this with an initial sparsity and an end sparsity, which states the percentage of low-impact parameters to remove by the end of the process. We then ran the model through a low-magnitude pruner function with these parameters so the model would identify parameters that did not impact the results and cut them out. After this, we performed the usual compiling and training with an additional parameter to update the pruning steps as the model is being trained. After the training, we calculated the accuracy, mean IoU, precision-recall values, and loss to determine if the model was still performing well after the pruning. Our loss came to a consistent 89%, the accuracy was 90%, and the mean IoU was 0.55. Compared to our unpruned model, our loss only rose by around 1% and the accuracy which was between 89-93% got up to 90%. This shows the inconsequentiality of the pruned parameters as removing them made very small changes to our loss and accuracy. Although our mean IoU went from 0.59 to 0.55, it staying above .50 means our model is still accurate enough to overlay blocks correctly so this drop is not too significant. Looking at the precision-recall curve, the drop at the beginning of the graph returned, but it still rose to the levels our unpruned model was at, displaying continued consistency. In regards to the prediction images, it was hard to find visible differences compared to the unpruned model performance, meaning our model remained nearly as accurate while taking up less computational space.

#### Once our pruning was a success, we performed quantization to compress our model into something operable on computers without access to high RAM. We started by creating the TensorFlow liteConverter and fitting it around our model. We then specified that we wanted the converter to preserve the default optimizations of our model and that all the parameters and activations should be 8 bits each. After this conversion, our model was small enough for use on more common computers. We then tested it with 10 more prediction images to visualize its new accuracy. After this, we saved and wrote the model to a directory so we were able to call it and make new predictions on test images. Once again, there was still little to no difference between the uncompressed and unpruned model from milestone 3 to our new model, even after going through pruning and quantization. In the end, our model became smaller and more usable, while still maintaining its accuracy and performance.

###### Comparing our previously tuned model to our compressed model in regards to Loss, IOU, and Precision-Recall:
![f1acbc19305c26e525dae6c1dca4ff24](https://user-images.githubusercontent.com/78321301/205547118-4521ed1f-f6e9-4d4f-8b0c-056eb1bbf7ef.png)

###### Ten segmented image examples from our compressed model:
![f5e02bb535b1ad5556202709978b535a](https://user-images.githubusercontent.com/78321301/205547087-d951b6a7-1698-4520-8104-abd611f5b8fd.png)
![3de47a0e282c2679f57b9c4f7a8da741](https://user-images.githubusercontent.com/78321301/205547092-e8721b12-d28c-4bc2-85e7-e76313467a5b.png)
![e7c77a22de8c342813f560f3ea58fc7c](https://user-images.githubusercontent.com/78321301/205547096-3a194af1-c3fb-4dab-9f7b-0f5d1f941c08.png)

#### Some challenges came with both pruning and quantizing our model. The pruning took some trial and error with the sparsity parameters because we didn’t want to cut the important weights from our model. Another challenge was knowing when to prune the model because doing it too soon would not cut out the useless parameters, but doing it too late would produce errors since the function was not compatible with an already compiled model. The perfect placement was right after creating the model and defining the optimizer, but right before compiling the model. For the quantization, with our model compressed, the images we gave it to predict on were no longer compatible, so we had to convert them from size float64 to float32. We also ran into several syntax errors when trying to graph the images since getting the predictions from the interpreter made them in a different format than what we used for the uncompressed model.

#### Throughout this project, we covered the important techniques of semantic segmentation. Going from environment preparation, to baseline performance segmentation, to hyperparameter optimization, and finally, model compression, which gave us an impressive model that can be used to get accurate and accessible predictions upon satellite imagery.
