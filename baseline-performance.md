### Nicholas Gutierrez & Sebastian Alcock
### CS301-101 - Introduction to Data Science
### November 6th, 2022

# Milestone 2: Semantic Segmentation of Satellite Imagery

#### In order to understand how we were able to successfully perform Semantic Segmentation, we will describe the code we utilized and its functions. Firstly, we begin by downloading our data onto Google Colab. This is accomplished by cloning our own remote GitHub Repository in which the images we are to segment are located. Once the repository is cloned, we iterate through the folder and create a sorted list of any file that ends with ‘jpg’ for images and ‘png’ for masks. Additionally, once we find an image or mask, we read its RGB values and resize it. Once added to the list, we utilize Patchify to extract and crop the image to multiple sub-images of appropriate size. With the images and masks finished and in their lists, we are now able to project an image and its corresponding mask with the imshow function. After this, the buildings, water, roads, and vegetation in the images need designated colors to differentiate between them as classes. To do this, we assign them hexadecimal and RGB values and place them in an array. This array is then run through the RGB_to_2D_label function to give us clean integer values so we can use them as labels to color the objects our program identifies. 
#### Moving into the training, we first utilize Keras to transform our color labels into a binary class matrix, and then we split our data into four subsets: X_train, X_test, y_train, and y_test. After choosing an even distribution for the weights, we were able to calculate the dice loss, focal loss, and total loss. Here we import and utilize the simple_multi_unet_model file from milestone-1 and its functions to define our model. Once our model is defined, we fit the model with our data and perform training and validation over 100 epochs. Through these iterations, we are able to visualize our loss, accuracy, and jacard coefficient.
#### With the training complete, we save the model and plot the loss as a function of the total epochs. In order to find the mean intersect over union, mean precision, and mean recall values, we load the model and save our loss function and coefficients as custom objects. Using the Keras IoU functions we can find the mean intersection over union. To calculate the precision-recall curve, we first need to normalize the test and prediction data. Next, we create a list of thresholds to iterate through, and for each threshold we calculate the precision and recall values for each class within the normalized dataset by utilizing the Tensorflow Keras Precision and Recall functions, from which we then find the prospective means by averaging the precision and recall for all classes. We then add these mean recall and mean precision values to two lists, of which we then plot to visualize the precision-recall curve. The last step is to use our trained data to predict some images that are hopefully similar in accuracy to the masks. After using model.predict() we plot the image, mask, and our prediction to view the results.

![0f2670cbc90b617797d2177a88ddec0f](https://user-images.githubusercontent.com/78321301/200209974-f82436d4-7bdf-4278-9071-99101c8e319d.png)

#### Our loss was around 89-91% and our accuracy rose to around 86-89% and the jacard coefficient averaged around 74-77%. In comparison, the validation loss consistently stayed at 90-91%, the validation accuracy was 84-86% and the jacard coefficient stayed around 71-73%. In the end, our mean IoU was about 0.602. When trying to minimize loss, an average of 86-89% means our program is quite likely to produce errors. This is dangerous since our accuracy is close to 90%, meaning we could get flawed results and mark them as successful. The graph of the precision recall has an initial drop off but rises up and smoothens out to an average of ~95%. What we can interpret from this curve is that a good mediation between precision and recall lies at lower threshold values. The precision and recall being generally high is a good sign that our program is producing relevant data and successfully categorizing classes. Looking at the graphs of our loss and IoU, we can see that as more epochs are completed, the algorithm becomes more accurate and the loss decreases as the mean intersection over union increases. Our mean IoU being 0.6 is a good result as it shows our predicted boxes have good overlap with the actual locations and we can see in the second graph that the overlap got better as the epochs went on. Finally, comparing our predicted images to the masks showed that our program was already producing results similar to the mask images with only some slightly irregular shapes. 

![56fb4c641daaf85d3f258ac6fd056903](https://user-images.githubusercontent.com/78321301/200151424-268d83e2-f385-4950-b629-e1c1034cc3d9.png)
![e2a8310662e47caabc59e8c0b738adfb](https://user-images.githubusercontent.com/78321301/200151425-12cc72c8-005e-47bb-9991-ecdce1cbcf25.png)
![73503a7500ba288f06651f2801211e9e](https://user-images.githubusercontent.com/78321301/200151428-f0a939b3-f0ab-4f3c-a616-621e7d305802.png)
![df28123d8b213378b2610e617fdc4ae4](https://user-images.githubusercontent.com/78321301/200151429-7d278be7-c5ed-46a8-85eb-40b7de877788.png)

#### Some of the challenges of this milestone came from adapting the starter code from UNet to Google Colab. We had to make several alterations throughout the code to make it run, and perform quite a bit of bug fixing too. Another roadblock was when trying to use the transfer flow precision and recall functions, our lists for prediction and test values could not be passed as parameters because they contained data that was initially greater than zero and did not work without normalizing the values first.
#### Looking at our results, we believe we made strong progress in our semantic segmentation and in our prediction algorithm. Our loss, however, definitely needs improvement and tuning since 89-91% loss means our program is still prone to errors. Moving forward, we plan to effectively utilize the TPE method to make our prediction formula more accurate and reduce our loss to better levels.
